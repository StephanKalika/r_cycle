---
title: "[Заметки по R](http://bdemeshev.github.io/r_cycle/): Мультиколлинеарность"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
lang: russian
---

Загружаем нужные пакеты:
```{r, message=FALSE}
library("knitr")
# opts_chunk$set(cache=TRUE)

library("ggplot2") # графики
library("glmnet") # LASSO
library("car") # compute vif()
library("MASS") # ridge regression
library("quantreg") # quantile regression
# install.packages("glmnet") # может быть нужно установить пакеты...
```


Возьмем встроенные в R данные по скорости автомобилей и длине тормозного пути.
Это данные 1920-х годов. Скорость в милях в час, миля --- это примерно $1.61$ километра.
Длина тормозного пути - в футах. Один фут - это примерно 30 сантиметров.
```{r}
h <- cars
ggplot(data = cars, aes(x = speed, y = dist)) + geom_point() +
    xlab("Скорость машины, миль в час") +
    ylab("Длина тормозного пути, футов") 
```


Построим полиномиальную регрессию тормозного пути от скорости
```{r}
m.poly3 <- lm(dist~speed+I(speed^2)+I(speed^3),data=h)
```

Предвосхищаем вопрос: Что значит `I(...)`? Буква `I` со скобками заставляет R воспринимать выражение внутри скобок в
естественном математическом смысле, а не в специальном "регрессионном" смысле.
Например: 

* `y~x+z` --- регрессия $y$ на $x$ и $z$
* `y~I(x+z)` --- регрессия $y$ на одну переменную $x+z$
* `y~(x+z+w)^2` --- регрессия $y$ на $x$, $z$, $w$ и все попарные произведения, т.е. на $xz$, $xw$ и $wz$.
* `y~I((x+z+w)^2)` --- регрессия на одну переменную $(x+z+w)^2$
* `y~(x+z+w)^3` --- регрессия $y$ на $x$, $z$, $w$, $xz$, $xw$, $zw$, $xzw$
* `y~I((x+z+w)^3)` --- регрессия на одну переменную $(x+z+w)^3$


Смотрим на отчёт по регрессии
```{r}
summary(m.poly3)
```

Мы видим, что:

* ни один коэффициент не значим
* регрессия в целом значима

Это один из признаков мультиколлинеарности.

Мультиколлинеарность --- ситуация, когда все предпосылки теоремы Гаусса-Маркова выполнены, но дисперсия оценок является слишком высокой по твоему мнению


Другие признаки мультиколлинеарности:
* Нестабильность коэффициентов при удалении одного наблюдения
* Высокие корреляции регрессоров
* Показатели в духе VIF и CI

Корреляционная матрица регрессоров:
```{r}
X <- model.matrix(~0+speed+I(speed^2)+I(speed^3),data=h)
cor(X)
```


Расчитаем индекс обусловленности, condition index,
\[
CI=\sqrt{\frac{\lambda_{max}}{\lambda_{min}}}
\]
где $\lambda$ --- собственное число матрицы $X'X$.

Вроде бы в некоторых источниках считают собственные числа у корреляционной или ковариационной матрицы регрессоров (???) (Лена Вакуленко) (Некоторые источники не указаны несколько дней. Вы можете улучшить эту статью :)

```{r}
X <- model.matrix(~speed+I(speed^2)+I(speed^3),data=h)
XX <- t(X) %*% X
eigen <- eigen(XX)
eigen$values
CI <- sqrt(max(eigen$values)/min(eigen$values))
CI
```

Наверняка есть пакет, в котором CI уже реализован. Но иногда дольше искать нужный пакет, чем написать функцию самому. Тем более тут нагляднее формула видна.

Википедия говорит, что $CI>30$ признак мультиколлинеарности. Но это не строгая граница. Мультиколлинеарность --- это субъективное недовольство высокими стандартными ошибками коэффициентов.


Рассчитаем коэффициент вздутия дисперсии, variance inflation factor
\[
VIF_j=\frac{1}{1-R_j^2}
\]
где $R_j^2$ --- это коэффициент $R^2$ в регрессии $j$-ой объясняющей переменной на остальные.

Популярная граница для VIF --- 10. У нас:
```{r}
vif(m.poly3)
```

NB: нужно уметь рассчитывать vif ручками, зная коэффициенты R^2 во вспомогательных регрессиях. Например:
```{r}
m.vif1 <- lm(speed~I(speed^2)+I(speed^3),data=h)
r2.1 <- summary(m.vif1)$r.squared
vif1 <- 1/(1-r2.1)
vif1
```

## Забудем про имеющуюся мультиколлинеарностьи попробуем заняться прогнозированием...

Из модели можно вытащить ковариационную матрицу оценок коэффициентов
```{r}
vcov(m.poly3)
```

Можно построить прогноз длины тормозного пути для машины с скоростью $speed=10$ миль в час.
```{r, "confint"}
new <- data.frame(speed=10)
predict(m.poly3,newdata=new,interval='confidence')
```

Если нужно вытащить стандартную ошибку для $\hat{y}$, то можно внутрь команды `predict` добавить опцию `se.fit=TRUE`. 

На всякий случай, есть два типа доверительных интервалов при прогнозировании. Доверительный интервал для мат. ожидания будущего игрека --- опция `'confidence'` в R. И прогнозный интервал для индивидуального значения игрека --- опция `'prediction'` в R.


Попробуем перейти к центированным переменным.
```{r}
h$spc <- h$speed - mean(h$speed)
m.poly3c <- lm(dist~spc+I(spc^2)+I(spc^3),data=h)
summary(m.poly3c)
```

Вау! Теперь появились значимые коэффициенты!!!!!

Но это всего лишь иллюзия. У этих оценок коэффициентов другая ковариационная матрица.
```{r}
vcov(m.poly3c)
```

И если построить доверительный интервал для прогнозов, то ничего не изменится.
```{r}
new$spc <- 10 - mean(h$speed)
predict(m.poly3c,newdata=new,interval='confidence')
```

Модель с центрированными переменными --- это всего лишь по-другому записанная исходная модель.

Кстати, нужно уметь строить доверительный интервал для прогноза "ручками" имея на руках оценки коэффициентов и оценку ковариационной матрицы.

## Как же бороться с мультиколлинеарностью?

* Смириться с высокими стандартными ошибками. Высокие? Ну и чёрт с ними!
* Увеличить количество наблюдений.
* Задать другой вопрос.

"I checked it very thoroughly," said the computer, "and that quite definitely is the answer. I think the problem, to be quite honest with you, is that you've never actually known what the question is." 

Douglas Adams, Hitchhiker's Guide to the Galaxy.


Обычная регрессия отвечает на такой вопрос: 

Чему равны несмещенные оценки с наименьшей дисперсией в модели
$E(y)=\beta_1+\beta_2 x +\beta_3 z$?

Можно задаться целью получить смещенные оценки. Почему нам интересны смещенные оценки? Дао учит нас, что любая смещенная оценка коэффициента $\beta$ есть несмещенная оценка какого-то коэффициента $\gamma$. 

Поэтому можно:

* Убрать из регрессии некоторые переменные
* В частности, можно оставить в регрессии "главные компоненты", искусственные переменные, которые содержат большую часть информации обо всех исходных переменных
* При минимизации RSS ввести штраф за слишком большие коэффициенты $\hat{\beta}$. Два популярных способа ввести штраф --- Ridge Regression и LASSO
* Выяснить как зависит медиана (или другой квантиль $y$) от объясняющих переменных


## Убрать из регрессии некоторые переменные

В регрессии $y$ на $x$ и $z$ коэффициент при $x$ показывает насколько изменится $y$ при изменении $x$ на единичку и фиксированном $z$. В регрессии $y$ на $x$ коэффициент при $x$ показывает насколько изменится $y$ при изменении $x$ на единичку и нефиксированном $z$.

```{r, "less variables"}
m.line <- lm(dist~speed,data=h)
summary(m.line)
```
## Метод главных компонент

В частности можно попытаться оставить только несколько переменных, которые "впитывают" в себя большую часть изменчивости всех исходных переменных. Этот метод называется метод главных компонент. Подробнее отдельно. 


## Ridge regression

Теоретически RR позволяет при удаче получить оценки $\hat{\beta}$ с меньшей, чем у OLS величиной среднеквадратичной ошибкой, MSE.

Алгоритм Ridge regression минимизирует функцию
\[
\sum_{i=1}^n (y_i-\hat{y}_i)^2+\lambda \sum_{j=1}^k \hat{\beta}_j^2
\]

По сравнению с обычной регрессией введен "штраф" $\lambda$ за большие значения $\hat{\beta}_j$.

Эту задачу можно решить в явном виде:
\[
\hat{\beta}_{RR}=(X'X+\lambda I)^{-1}X'y$
\]

По другому RR можно сформулировать так: минимизируем обычный RSS при ограничении $\sum_{j=1}^k \hat{\beta}_j^2 \leq c$. Выписав функцию Лагранжа для этой задачи минимизации получим формулировку задачи RR с лямбдой.

Делаем RR в R  :)
```{r}
lambdas <- seq(0.1,10,by=0.1) # для этих лямбд будем делать RR
m.rr <- lm.ridge(dist~speed+I(speed^2)+I(speed^3),lambda=lambdas,data=h)
```

Посмотрим, как зависят оценки коэффициентов, от штрафа за высокие значения $\hat{\beta}$:
```{r}
head(coef(m.rr))
plot(m.rr)
```

График не оцень информативен, т.к. масштаб коэффициентов существенно отличается. 

Не вдаваясь в подробности скажем, что есть много алгоритмов, определяющих, какой $\lambda$ выбрать. Помимо кросс-валидации популярны еще:
```{r}
m.rr$kHKB # оценка Hoerl, Kennard and Baldwin
m.rr$kLW # оценка Lawless and Wang 
```

Достоинства RR:
* Оценка RR всегда есть. Даже в случае жесткой мультиколлинеарности. Даже если регрессоров больше чем, наблюдений.
* Теорема: существует $\lambda$, при котором $MSE(\hat{\beta}_{RR})$  строго меньше, чем $MSE(\hat{\beta}_{OLS})$. Напомню, что MSE --- mean squared error, это $MSE(\hat{\beta})=E((\hat{\beta}-\beta)^2)$. Оценки $\hat{\lambda}_{HKB}$ и $\hat{\lambda}_{LW}$ пытаются поймать это неизвестное $\lambda$ минимизирующее $MSE$.



Недостатки RR:
* Нужно выбирать $\lambda$. Конечно, есть куча алгоритмов, как это сделать. 
* Насколько мне известно, удачных методов постоения доверительных интервалов для $\beta$ нет.

## LASSO
LASSO очень похож на RR, отличие состоит в том, что в LASSO минимизируется функция
\[
\sum_{i=1}^n (y_i-\hat{y}_i)^2+\lambda \sum_{j=1}^k |\hat{\beta}_j|
\]

Заметим сразу, что функция "мерзкая": в ней есть модуль, поэтому она не дифференциируема. Поэтому явной готовой формулы для $\hat{\beta}_{LASSO}$ ожидать не приходится.

По другому LASSO можно сформулировать так: минимизируем обычный RSS при ограничении $\sum_{j=1}^k |\hat{\beta}_j| \leq c$. Выписав функцию Лагранжа для этой задачи минимизации получим формулировку задачи LASSO с лямбдой.

Достоинства LASSO:
* В угловых решениях некоторые $\hat{\beta}_j$ в точности оказываются равными нулю. Т.е. LASSO без всякой проверки гипотез в каком-то смысле делит коэффициенты на "существенные" и "несущественные"
* Оценка LASSO всегда есть. Даже в случае жесткой мультиколлинеарности. Даже если регрессоров больше чем, наблюдений.

Недостатки LASSO:
* Нужно выбирать $\lambda$. Конечно, есть куча алгоритмов, как это сделать. 
* Насколько мне известно, удачных методов постоения доверительных интервалов для $\beta$ нет.

LASSO в R. Сначала надо ручками сваять матрицу регрессоров без единичного столбца.
```{r}
X <- model.matrix(~0+speed+I(speed^2)+I(speed^3),data=cars)
y <- h$dist
fit <- glmnet(X,y,alpha=0)
plot(fit)
```

Из результатов оценивания LASSO можно вытащить:
```{r}
head(fit$lambda) # вектор лямбд
head(fit$a0) # вектор оценок свободных коэффициентов
```

И оценки остальных коэффициентов для каждого лямбда
```{r}
fit$beta[,1:5] # покажем только первые 5
```

Выбираем оптимальное лямбда с помощью кросс-валидации (без подробностей)
```{r}
cv.fit <- cv.glmnet(X,y)
cv.fit$lambda.min
```

## Квантильная регрессия

Пытается получить состоятельные оценки коэффициентов у уравнении:

$q_{\alpha}(y)=\beta_1+\beta_2 x +\beta_3 z$

Здесь $q_{\alpha}$ - это квантиль порядка $\alpha$

```{r}
m.poly3q <- rq(dist~speed+I(speed^2)+I(speed^3),data=h,tau=c(0.1,0.5,0.9))
plot(m.poly3q)
```

В квантильной регрессии нет простых явных формул для оценок дисперсий коэффициентов. Один из популярных способов получить их --- бутстрэп. Без подробностей.
```{r, "boot se"}
summary(m.poly3q,se='boot')
```

Изобразим на одном графике линию обычной регрессии и линии квантильных регрессий.
```{r, "quantile graph"}
# стащить из коэнкэра клёвый график :)
```






