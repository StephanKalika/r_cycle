---
title: "[Заметки по R](http://bdemeshev.github.io/r_cycle/): Метод опорных векторов (Support Vector Machine)"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
lang: russian
---

Загружаем нужные пакеты:
```{r, message=FALSE}
library("knitr")
# opts_chunk$set(cache=FALSE)

library("ggplot2") # графики
library("kernlab") # Support Vector Machines
# install.packages("kernlab") # может быть нужно установить пакеты...
library("dplyr")
library("caret")
```

Другой популярный пакет для SVM в R --- это `e1071`.


## Ядрёная функция!

Ядерная функция, или просто ядро, --- это скалярное произведение в преобразованном пространстве. 

\[
K(x,x')=(\phi(x),\phi(x'))
\]


```{r}
x <- 1:6
y <- 1:6
k1 <- vanilladot()
k2 <- rbfdot(sigma=1)
k1(x,y)
k2(x,y)
```

Скалярное произведение трудно интерпретировать "в лоб". Лучшая известная мне интерпретация: скалярное произведение --- это произведение длин векторов на косинус угла между ними, $(y,y')=|y|\cdot  |y'| \cos(y,y')$. 

Скалярное произведение содержит в себе всю информацию о геометрии пространства. Если знать чему равно скалярное произведение между любыми векторами, то можно посчитать длину любого вектора и угол между любыми векторами. 


## Теория SVM


SVM пытается разделить гиперплоскостью наши данные в более многомерном пространстве, чем исходное, [видео](http://www.youtube.com/watch?&v=3liCbRZPrZA). Это новое пространство называется спрямляющим.

Для гауссовского ядра это спрямляющее пространство будет бесконечномерным.

## Оцениваем SVM с заданными параметрами

Загружаем данные по стоимости квартир в Москве:
```{r}
filename <- "~/Documents/em301/datasets/flats_moscow.txt"
h <- read.table(filename,header=TRUE)
str(h)
```


Для ksvm нужно указать, что зависимая переменная --- факторная, а не количественная. Более того, названия категорий должны быть валидными именами для переменных.
```{r}
h$brick <- as.factor(h$brick)
levels(h$brick) <- list(no="0", yes="1") 
h$walk <- as.factor(h$walk)

```


Разделим выборку на две части --- 75% для обучения и 25% для оценки качества обучения. Указание зависимой переменной важно, т.к. R случайно отберет 75% единичек и 75% нулей в обучающую выборку.
```{r}
set.seed(777)
train.index <- createDataPartition(y = h$brick,p=0.75,list=FALSE)
train.h <- h[train.index,]
test.h <- h[-train.index,]
```

Команда `set.seed` обеспечивает воспроизводимость эксперимента на другом компьютере. 


Находим оптимальную разделяющую гиперплоскость
```{r}
m1 <- ksvm(brick~price+totsp+livesp+kitsp+dist+metrdist+walk+floor+code,data=train.h,kernel="rbfdot",kpar=list(sigma=0.05),C=5)
```


Настраивать можно:
* Тип ядра, `kernel="rbfdot"`
* Параметры ядра, если они у него есть, `kpar=list(sigma=0.05)`
* Величину штрафа за неправильно классифицированные объекты, `C=5`

Есть встроенные методы подбора параметра $\sigma$ для Гауссовского ядра:
```{r}
m1 <- ksvm(brick~price+totsp+livesp+kitsp+dist+metrdist+walk+floor+code,data=train.h,kernel="rbfdot",C=5)
m1
```

Слабо добраться до оптимального автоматически найденного $\sigma$?
```{r}
kernelf(m1) # описание ядра: тип и параметры
kpar(kernelf(m1))$sigma # из описания извлекаем параметры, из них --- сигму
```


Вектора, лежащие на границе разделяющей полосы, называются опорными.

Найденная оптимальная гиперплоскость не даёт нам простой интепретации зависимости. Из объекта `m1` можно  извлечь опорные вектора, но в реальной практической задаче их как минимум сотни, и что с ними делать в такой ситуации мне не известно.



## Прогнозы и прогнозные вероятности


Строим прогнозы для тестовой части выборки:
```{r}
test.h$brick.pred <- predict(m1,test.h)
table(test.h$brick.pred,test.h$brick)
```

Также можно спрогнозировать расстояние до разделяющей гиперплоскости...

...

Скомбинировав SVM и logit, можно в каком-то смысл оценить $P(y_i=1)$ с помощью SVM. Сначала нужно построить прогноз по SVM для всех объектов в выборке. (тут точнее...) Используя расстояние от каждого прогноза до разделяющей гиперплоскости мы оценим модель похожую на logit...

Этот подход уже автоматизирован:
```{r}
m1 <- ksvm(brick~price+totsp+livesp+kitsp+dist+metrdist+walk+floor+code,data=train.h,kernel="rbfdot",kpar=list(sigma=0.05),C=5,prob.model=TRUE)

probs <- predict(m1, test.h,type="probabilities")
head(probs)
```

Используя этот подход можно построить и кривые предельных эффектов.





## Няшные графики

Красивые графики, как мне кажется, можно получить только для случая двух объясняющих переменных

Пример из документации:
```{r}
x <- rbind(matrix(rnorm(120),ncol=2),matrix(rnorm(120,mean=3),ncol=2))
y <- matrix(c(rep(1,60),rep(-1,60)))
 
svp <- ksvm(x,y,type="C-svc")
plot(svp,data=x)
```

Закрашенные треугольники и кружочки --- это опорные вектора, т.е. те точки, которые оказались на границе разделяющей полосы в спрямляющем пространстве.


## Выбор между точностью подгонки и простотой модели

Во многих моделях есть параметр, отвечающий за простоту модели.  Чем проще модель, тем хуже модель описывает выборку, по которой она оценивалась.

В линейной регрессии параметр сложности модели --- это количество регрессоров, $k$. Чем больше регрессоров, тем ниже сумма квадратов остатков, $RSS$.

На гистограмме параметр сложности --- число столбцов.

В ridge regression и LASSO параметр простоты --- это $\lambda$. Действительно, LASSO минимизирует
\[
\sum_{i=1}^n (y_i-\hat{y}_i)^2+\lambda \sum_{j=1}^k |\hat{\beta}_j|
\]
Значит, чем больше параметр $\lambda$, тем больше стремление алгоритма LASSO занулить некоторые $\hat{\beta}_j$.

В SVM за "простоту" отвечают $\sigma$ и $C$.

Этот параметр сложности не так легко оценить, как коэффициенты модели. Если наивно попытаться выбрать параметр сложности так, чтобы модель как можно лучше описывала бы выборку, по которой она оценивалась... То ничего хорошего не выйдет. Окажется, что оптимальное количество регрессоров равно плюс бесконечности, а оптимальное $\lambda$ в LASSO равно нулю. В регрессии одно из решений этой проблемы --- это проверка гипотез о значимости коэффициентов.

Есть и универсальный способ выбора сложности модели --- кросс валидация (перекрёстная проверка, cross validation). Её идея состоит в том, что надо оценивать качество прогнозов не по той же выборке, на которой оценивалась модель, а на новых наблюдениях.

### k-кратная кросс-валидация

* Разбиваем случайным образом всю выборку на k частей. Сразу мораль: используйте `set.seed()` для воспроизводимости эксперимента.
* Прогнозы для первой части выборки строим, оценивая модель по наблюдениям всех остальных частей. Прогнозы для второй части выборки строим, оценивая модель по наблюдениям всех частей кроме второй. И так далее. 
* Получаем по одному прогнозу для каждого наблюдения. 
* Считаем сумму квадратов ошибок прогнозов или другой показатель их качества.

Популярные значения k:
* 10-кратная кроссвалидация
* k равно числу наблюдений. Т.е. модель оценивается по всем наблюдениям кроме одного. Для этого невключенного наблюдения считается ошибка прогноза. Исключая по очереди то одно, то другое наблюдение, получаем ошибку прогноза для каждого наблюдения. По этим ошибкам считаем сумму квадратов. При этом подходе алгоритм не является случайным и зачастую есть готовые формулы для суммы квадратов ошибок прогнозов.


### Кросс-валидация на примере SVM



Создаём табличку перебираемых $C$ и $\sigma$:
```{r}
C <- c(1,10,100)
sigma <- c(0.1,1,10)
d <- expand.grid(C,sigma) # Случайно не декартово ли произведение это? :)
colnames(d) <- c("C","sigma")
head(d)
```

Пробуем 10-кратную кросс-валидацию для конкретных $C$ и $\sigma$:
```{r}
set.seed(33222233) # любимый seed Фрекен Бок
m1 <- ksvm(data=h,brick~price+totsp+livesp+kitsp+dist+metrdist+walk+floor+code,
           kernel=rbfdot,
           kpar=list(sigma=1),
           C=1,cross=10)
cross(m1) # cross validation error
```


Для удобства оформляем это в функцию двух переменных
```{r}
f_cross <- function(sig,C) {
  model <- ksvm(data=h,
                brick~price+totsp+livesp+kitsp+dist+metrdist+walk+floor+code,
             kernel=rbfdot,
             kpar=list(sigma=sig),
             C=C,cross=10)
  return(cross(model))

}
f_cross(1,1) # тестируем сделанную функцию
```

Применяем её ко всем возможным $C$ и $\sigma$
```{r}
d <- mutate(d, cr = f_cross(sigma,C))
head(d)
```

## Вкусная Морковка

Есть замечательный пакет `caret` созвучный с carrot :) В нем реализовано и деление выборки на тестовую и обучающую части и подбор параметров  с помощью кросс-валидации для кучи методов, в том числе для SVM. Морковка очень вкусная :)

```{r, warning=FALSE}
library(caret)
ctrl <- trainControl(classProbs=TRUE)
fit <- train(brick~price+totsp+livesp+kitsp+dist+metrdist+walk+floor+code,
             data=train.h,method="svmRadial",trControl = ctrl)
fit
```

Можно прогнозировать с помощью наилучшей модели
```{r}
head(predict(fit, test.h ,type="prob"))
```


### Недостатки SVM в задаче классификации

* Оптимальная разделяющая гиперплоскость не даёт простой формулы, описывающей зависимость $y$ от регрессоров. Нет оценок, которые бы легко интерпретировались.
* Напрямую SVM применима только для двух классов. Есть способы расширить её на много классов.



### Как применить SVM, если $y$ принимает больше двух значений?

Если $y$ принимает больше двух значений, то можно использовать метод "все против всех":
* Для каждой пары возможных значений $y$, $y=a$ и $y=b$, составить выборку, содержащую только эти значения $y$
* На каждой из этих выборок запустить SVM, которая будет выбирать из двух значений $y$ одно
* В результате мы получили кворум SVM-ок, голосующих каждая за свой прогноз $y$. Выбираем прогноз простым большинством голосов.

Естественно, это уже автоматизировано. Есть и другие методы.

### SVM на практике

Чайники используют следующую процедуру оценивания SVM:

* Оценить несколько SVM с параметрами "от фонаря"
* Выбрать наилучшую

Суровые челябинские жители советают делать так:

* Нормировать все переменные. Например, можно вычитать среднее и делить на корень из дисперсии. Можно все переменные привести к диапазону $[0;1]$. Гуру постигшие абсолют рекомендуют диапазон $[0;10]$, как более понятный простым смертным.
* Поделить выборку на две части, обучающую и тестовую
* По обучающей части оценить SVM с гауссовским ядром, `rbfdot` в пакете `kernlab`
* Параметры $C$ и $\sigma$ подобрать с помощью кросс-валидации
* Оценить прогнозную силу полученной модели по тестовой выборке
* Если набор данных слишком большой и кросс-валидация занимает _не меряно_ времени, то её можно делать на небольшой случайной подвыборке обучающей части.

### Ссылки:

* [A Practical Guide to Support Vector Classiffication](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf), отсюда взята инструкция по практике SVM 

* [A User's Guide to Support Vector Machines](http://pyml.sourceforge.net/doc/howto.pdf) --- картинки, помогающие понять разницу гауссовского ядра для разных $C$ и $\sigma$ с примерами на питоне. Питон --- это еще один подходящий язык програмирования для анализа данных.

* Пакет caret: [официальный сайт](http://caret.r-forge.r-project.org/), [статья  в JSS](http://www.jstatsoft.org/v28/i05/paper), и [введение побольше](http://cran.cermin.lipi.go.id/web/packages/caret/vignettes/caretTrain.pdf). Пакет достаточно активно обновляется и это радует :)


