---
title: "[Заметки по R](http://bdemeshev.github.io/r_cycle/): Метод главных компонент"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
lang: russian
---

Метод главных компонент позволяет заменить несколько исходных переменных на меньшее количество новых переменных. Новые искусственные переменные называются главными компонентами.

Существует два эквивалентных взгляда на метод главных компонент. 

1. Последовательная максимизация выборочного разброса новых переменных:
  - Новые переменные являются линейными комбинациями старых
  - Новые переменные ортогональны
  - Первая главная компонента имеет максимальную выборочную дисперсию
  - Вторая главная компонента имеет максимальную выборочную дисперсию при фиксированной первой
  - Третья главная компонента имеет максимальную выборочную дисперсию при фиксированной первой и второй
  - и т.д.
  
2. Последовательная минимизация суммы квадратов расстояний от старых точек до новых точек:
  - Расположим новые точки на прямой так, чтобы сумма квадратов расстояний до старых была минимальна
  - Расположим новые точки на плоскости так, чтобы сумма квадратов расстояний до старых была минимальна
  - Расположим новые точки в трехмерном подпространстве так, чтобы сумма квадратов расстояний до старых была минимальна
  - и т.д.
  - Зададим систему координат, так чтобы они были ортогональны и первая совпадала с прямой, первые две --- с плоскостью, первые три с найденным трёхмерным подространством и т.д.
  
Формально: 

Мы хотим вместо $k$ переменных оставить $p<k$. Другими словами, хотим заменить матрицу $X_{n\times k}$ на матрицу $\hat{X}_{n\times k}$ ранга $p$, так чтобы минимизировать сумму квадратов ошибок:
\[
\min \sum_{j=1}^k \sum_{i=1}^n (x_{ij}-\hat{x}_{ij})^2
\]


### Частный случай: от двух переменных к одной

Что делает регрессия $y$ на $x$? Строит прямую так, чтобы сумма квадратов вертикальных отклонений точек от прямой была бы минимальна.

Что делает регрессия $x$ на $y$? Строит прямую так, чтобы сумма квадратов горизонтальных отклонений точек от прямой была бы минимальна.

Что делает выделение главной компоненты пары векторов $x$ и $y$? Строит прямую так, чтобы сумма квадратов отклонений точек от прямой по перпендикуляру была бы минимальна. И от двух координат на плоскости переходим к одной координате на прямой.

> Шампур --- это первая главная компонента шашлыка!

### Метод главных компонент на языке линейной алгебры

про квадратичные формы

про собственные числа и собственные векторы

про SVD


### В R всё просто и быстро:

Загружаем нужные пакеты:
```{r, message=FALSE}
library("knitr")
# opts_chunk$set(cache=TRUE) # кэшируем фрагменты кода

library("ggplot2") # для построения графиков
library("HSAUR") # нам нужен только набор данных по многоборью
# install.packages("HSAUR") # может быть нужно установить пакеты...

opts_chunk$set(fig.align = 'center') 
options(width = 110)
theme_set(theme_bw())
```

Загружаем интересующий нас набор данных по многоборью:
```{r}
data(heptathlon)
head(heptathlon)
```

Корреляции исходных переменных:
```{r}
print(cor(heptathlon), digits = 2)
```

Восьмой столбец, `score`, нам не нужен, т.к. это не результат соревнования, а итоговый балл по многоборью. Применяем метод главных компонент к стандартизированным переменным. Стандартизация здесь нужна, так как разные переменные измерены в несопоставимых единицах измерения:
```{r}
h <- heptathlon[, -8]
hept_pca <- prcomp(h, scale = TRUE)
```

Теперь можно раздобыть сами главные компоненты:
```{r}
pc <- hept_pca$x
head(pc)
```

Веса исходных переменных в главных компонентах:
```{r}
loadings <- hept_pca$rotation
head(loadings)
```

Посмотрим как первая компонента связана с итоговым результатом спортсмена:
```{r}
qplot(x = heptathlon$score, y = pc[, 1]) + 
  labs(x = "Сумма баллов по многоборью", 
       y = "Первая главная компонента",
       title = "Связь первой компоненты с итоговым результатом")
cor(heptathlon$score, pc[, 1])
```

Мораль такая: не зная способ подсчета очков за семиборье мы его восстановили! Первая главная компонента вбирает в себя максимум отличий между спортсменами :)

Смотрим какую долю дисперсии объясняют главные компоненты:
```{r}
summary(hept_pca)
```

Для построения графика сначала получим вектор долей дисперсии, объясняемых переменными:
```{r}
exp_percent <- summary(hept_pca)$importance[2, ]
qplot(y = exp_percent, x = names(exp_percent)) + 
  geom_bar(stat = 'identity') +
  labs(x = "Главные компоненты",
       y = "Доли диспесии",
       title = "Доли дисперсии, объясняемые главными компонентами \n (без предварительной стандартизации)")
```


График первых двух компонент с вкладом каждой переменной:
```{r, fig.height=5, fig.width=6}
biplot(hept_pca, 
       xlim = c(-0.8, 0.8), ylim = c(-0.6, 0.6))
```

Красные векторы --- это проекции единичных (?) векторов исходных координат на плоскость первых двух главных компонент.

На этом графике видно, например, что:

* Спортсменка Launa из Папуа-Новой Гвинеи не такая как все!
* С ростом результата прыжка в длину, `longjump`, первая главная компонента падает, а вторая --- практически не изменяется.
* Переменные `longjump` и `hurdles` предположительно отрицательно коррелированы, т. к. с ростом первой главной компоненты меняются в разные стороны.

### Вращение

После PCA иногда дополнительно вращают главные компоненты. Это уже выходит не PCA, а PCA плюс вращение! Кажется, SPSS делает по дефолту PCA + вращение, поэтому результаты не совпадают с R.

Здесь про varimax



Почиташки:

* [SVD and PCA on stats SE](http://stats.stackexchange.com/questions/134282)
* [SVD vs eigen on stats SE](http://stats.stackexchange.com/questions/79043)
* [стрелочки на biplot on stats SE](http://stats.stackexchange.com/questions/27080)
* [varimax on stats SE](http://stats.stackexchange.com/questions/59213) и [PCA + varimax это не PCA](http://stats.stackexchange.com/questions/612)
